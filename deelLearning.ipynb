{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory usage\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========LOADING DATA========\n",
      "(569537, 293)\n",
      "========COMPLETE LOADING DATA========\n"
     ]
    }
   ],
   "source": [
    "print('========LOADING DATA========')\n",
    "df_500 = pd.read_csv('pct1_cal/modified_alter_alphas_066_labels_500.csv')\n",
    "print(df_500.shape)\n",
    "print('========COMPLETE LOADING DATA========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('pct1_cal' + '/500*')\n",
    "for file in filenames:\n",
    "    df = pd.read_csv(file)\n",
    "    #print(df.shape)\n",
    "    dates = df.tradeDate.sort_values().unique()\n",
    "    total = 0\n",
    "    for date in dates:\n",
    "        temp_df = df[df.tradeDate == date]\n",
    "        temp_IC = temp_df[['y', 'y_pred']].corr().iloc[0,1]\n",
    "        total += temp_IC\n",
    "    IC = total/len(list(dates)) #df[['y', 'y_pred']].corr().iloc[0,1]\n",
    "    print('{}: {}'.format(file[9:-4], IC))\n",
    "# 500_Alter_066_full_XGBoostR-openclose_pct1_rank-r-25--99: 0.030344625575579553\n",
    "# 500_Alter_066_full_DecisionTreeR-openclose_pct1_rank-r-25--99: 0.01931140387478804\n",
    "# 500_Alter_066_full_LGBMRegressor-openclose_pct1_rank-r-25--99: 0.03376780851376303\n",
    "# 500_Alter_066_full_RidgeR-openclose_pct1_rank-r-25--99: 0.03796098294064469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_index = ['ticker', 'tradeDate']\n",
    "f_x_036_222 = pickle.load(open('pct5_cal/f_x_036_222', 'rb'))\n",
    "f_222 = pickle.load(open('pct5_cal/f_alphas_222', 'rb'))\n",
    "f_x = [x for x in f_x_036_222 if x not in f_222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_y = ['PCT5_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_036 = out[f_index + f_x + f_y]\n",
    "df_036 = df_036.to_csv('pct5_cal/modified_alter_alphas_036.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_500.tradeDate.sort_values().unique()\n",
    "epoch_ts = list(dates)\n",
    "\n",
    "f_index = ['ticker', 'tradeDate']\n",
    "f_x = pickle.load(open(\"pct1_cal/f_x_066\", \"rb\"))\n",
    "label_list = ['PCT5_rank', 'PCT2_rank', 'openclose_pct1_rank', 'askbid_pct1_rank']\n",
    "f_y = label_list[2]\n",
    "\n",
    "if_pcas = ['', 'pca'] # pca 或者空字符串# 是否做PCA\n",
    "if_pca = if_pcas[0] # pca 或者空字符串\n",
    "pca_components_list = [0.99, 0.95, 0.90, 0.85, 0.80]\n",
    "pca_components = pca_components_list[0]\n",
    "num_leaves_list = [10, 15, 20, 25, 30]\n",
    "num_leaves = num_leaves_list[3]\n",
    "depth_list = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "depth = depth_list[3]\n",
    "\n",
    "model_list = ['RidgeR', 'DecisionTreeR', 'XGBoostR', 'LGBMRegressor', 'RandomForestR']\n",
    "result = {}\n",
    "\n",
    "for model_name in model_list:\n",
    "#def run_num(model_name):\n",
    "#for f_y in label_list:\n",
    "    print('======== LEN_TRAIN {} ========'.format(f_y))\n",
    "    target_types = ['r', 'c'] # 分类问题还是回归问题 r 回归问题 c 分类问题\n",
    "    target_type = target_types[0]\n",
    "\n",
    "    result_name = '500_Alter_066_full_{}-{}-{}-{}-{}-{}'.format(model_name, f_y, target_type, num_leaves, if_pca, int(100*pca_components))\n",
    "    print(result_name)\n",
    "\n",
    "    update = 22 # 训练长度：22天\n",
    "    train_si = epoch_ts.index('2017-01-03') # included. '2017-01-03'\n",
    "    train_ei = epoch_ts.index('2019-01-02') # excluded. '2018-12-28'\n",
    "    test_si = epoch_ts.index('2019-01-02') # included. '2019-01-02'\n",
    "    test_ei = epoch_ts.index('2019-02-01') # excluded. '2019-01-31'\n",
    "    test_fi = len(epoch_ts) - 1 # excluded.\n",
    "\n",
    "    # number of epochs，循环次数\n",
    "    num_epoch = round((test_fi - test_ei) / 22)\n",
    "    epoch_range = range(0, num_epoch + 1)\n",
    "    #epoch_range = range(0, 1)\n",
    "\n",
    "    start = time.time()\n",
    "    df_result_all = pd.DataFrame()\n",
    "    for epoch in epoch_range:\n",
    "        print('----- EPOCH {}------'.format(epoch))\n",
    "        update_n = epoch * update\n",
    "        # get a list of train dates\n",
    "        epoch_t_train = epoch_ts[train_si + update_n : train_ei + update_n]\n",
    "        # get a list of test dates\n",
    "        epoch_t_test = epoch_ts[test_si + update_n : test_ei + update_n]\n",
    "        df_train = df_500[df_500.tradeDate.apply(lambda x: x in epoch_t_train)].reset_index(drop=True)\n",
    "        df_test = df_500[df_500.tradeDate.apply(lambda x: x in epoch_t_test)].reset_index(drop=True)\n",
    "        print('预测时间：', epoch_t_test)\n",
    "        print('数据大小：', df_train.shape, df_test.shape)\n",
    "\n",
    "        # 获得 x\n",
    "        # PCA处理\n",
    "        if if_pca == 'pca':\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=pca_components)\n",
    "            pca.fit(df_train[f_x])\n",
    "            x_train = pca.transform(df_train[f_x])\n",
    "            x_test = pca.transform(df_test[f_x])\n",
    "        else:\n",
    "            x_train = df_train[f_x].values\n",
    "            x_test = df_test[f_x].values\n",
    "        print('处理后x：', x_train.shape, x_test.shape)\n",
    "\n",
    "        # 获得y\n",
    "        y_train = df_train[f_y].copy()\n",
    "        y_test = df_test[f_y].copy()\n",
    "        print('处理后y：', y_train.shape, y_test.shape)\n",
    "\n",
    "        if model_name=='RidgeR': # alpha: 200\n",
    "            from sklearn.linear_model import Ridge\n",
    "            model = Ridge(alpha=1)\n",
    "            model.fit(x_train, y_train)\n",
    "        elif model_name=='DecisionTreeR': # 'splitter': ['random'], 'criterion': ['friedman_mse'], 'max_depth': [6], 'min_samples_leaf': [41], 'min_impurity_decrease': [0.5]\n",
    "            from sklearn.tree import DecisionTreeRegressor\n",
    "            model = DecisionTreeRegressor(splitter = 'random', criterion = 'friedman_mse', max_depth = 6, min_samples_leaf = 41, min_impurity_decrease = 0.5)\n",
    "            model.fit(x_train, y_train)\n",
    "        elif model_name=='RandomForestR': # 'n_estimators': [400], 'max_depth': [9], 'max_features': [29]\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            model = RandomForestRegressor(n_estimators=400, max_depth=9, max_features=29)\n",
    "            model.fit(x_train, y_train)\n",
    "        elif model_name == 'XGBoostR': # 'n_estimators': [20], 'max_depth': [3], 'max_features': [10], 'subsample': [1.0]},\n",
    "            from xgboost import XGBRegressor\n",
    "            model = XGBRegressor(n_estimators=20, max_depth=6, subsample = 1.0)\n",
    "            model.fit(x_train, y_train)\n",
    "        elif model_name=='LGBMRegressor':\n",
    "            model = lgb.LGBMRegressor(learning_rate=0.09, num_leaves = num_leaves, max_depth=depth)\n",
    "            model.fit(x_train, y_train, eval_set=[(x_train,y_train), (x_test,y_test)], eval_metric='l2')\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        # 获得结果\n",
    "        print('get result')\n",
    "        df_result = df_test[f_index].copy()\n",
    "        df_result['y'] = y_test\n",
    "        df_result['y_pred'] = y_pred\n",
    "        df_result_all = df_result_all.append(df_result)\n",
    "\n",
    "    print(f'耗时:{time.time() - start}') \n",
    "    print('sort values')\n",
    "    df_result_all = df_result_all.sort_values(by=['ticker', 'tradeDate']).reset_index(drop=True)\n",
    "    IC = df_result_all[['y', 'y_pred']].corr().iloc[0,1]\n",
    "    result[model_name] = IC\n",
    "    print('store data')\n",
    "    df_result_all.to_csv('pct1_cal/{}.csv'.format(result_name), index=False)\n",
    "    print('======== COMPLETE {} ========'.format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
