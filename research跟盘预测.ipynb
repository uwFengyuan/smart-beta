{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import joblib\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from influxdb import InfluxDBClient\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = '/data/liufengyuan'\n",
    "f_index = ['ticker', 'tradeDate']\n",
    "f_x_221 = pickle.load(open(file_location + '/pct1_cal/dailyprediction/f_x_221', 'rb'))\n",
    "extra_factors = ['volume', 'cachgPct', 'thecommittee', 'askVolume1',\n",
    "       'bidVolume1', 'caQrr', 'caTr', 'OCVP1',\n",
    "       'Open/vwap-1', 'Gap']\n",
    "_extra_factors = ['jhjj_FACTORS_' + x for x in extra_factors]\n",
    "f_x = f_x_221.copy()\n",
    "f_x.extend(_extra_factors)\n",
    "print(len(f_x_221), len(f_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "#pct1_label = pd.read_csv(file_location + '/raw_data/pct1_8_12_open.csv')\n",
    "#pct1_label['pct1_open_rank'] = pct1_label.groupby('tradeDate')['pct1_open'].rank(pct = True)\n",
    "#pct1_label.to_csv(file_location + '/labels/{}.csv'.format('pct1_open'), index=False)\n",
    "pct1_label = pd.read_csv('/data/liufengyuan/labels/pct1_open.csv')\n",
    "pct1_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2017-01-03 - 2022-07-07 alter 221 + jhjj 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter221_jhjj_data = pd.read_csv(file_location + '/pct1_cal/每日预测/221_jingjia_data.csv')\n",
    "alter221_jhjj_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* alter 221 20220427-20220804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_factor(file_loc, name, date):\n",
    "    alpha = pd.read_csv(file_loc + name + '.csv')\n",
    "    if alpha.columns.values.tolist()[0] == 'date':\n",
    "        alpha = alpha.rename(columns={\"date\": \"tradeDate\"})\n",
    "    alpha = alpha.set_index(['tradeDate']).stack().reset_index().rename(columns={'level_1': 'ticker', 0: name})\n",
    "    alpha['ticker'] = alpha['ticker'].astype(int)\n",
    "    if date != '':\n",
    "        alpha = alpha[alpha.tradeDate == date]\n",
    "    alpha = alpha.set_index(['ticker', 'tradeDate'])\n",
    "    alpha = alpha.sort_index(ascending=True)\n",
    "    return alpha\n",
    "\n",
    "# combine all factors\n",
    "def combine_factors(file_loc, f_x, date, thresh):        \n",
    "    parallel_obj = Parallel(n_jobs=-1)(delayed(each_factor)(file_loc, name, date) for name in f_x)\n",
    "    result = pd.concat(parallel_obj, axis=1)\n",
    "    result = result.reset_index()\n",
    "    result = result.sort_values(by=['ticker', 'tradeDate']).reset_index(drop=True)\n",
    "    result = result.dropna(thresh=thresh)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取昨天的普通因子\n",
    "old_factors = combine_factors(file_location + '/pct1_cal/每日预测/data/', f_x_221, '', 67)\n",
    "old_factors = old_factors[old_factors.tradeDate >= '2022-07-08'].reset_index(drop=True)\n",
    "old_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jingjia(filenames):\n",
    "    result_list = []\n",
    "    for name in filenames:\n",
    "        #print(name)\n",
    "        alpha = pd.read_csv(name)\n",
    "        alpha = alpha.drop(columns='Unnamed: 0')\n",
    "        alpha  = alpha.rename(columns={\"dataDate\": \"tradeDate\"})\n",
    "        alpha['ticker'] = alpha['ticker'].astype(int)\n",
    "        alpha = alpha.set_index(['ticker', 'tradeDate'])\n",
    "        alpha = alpha.sort_index(ascending=True)\n",
    "        result_list.append(alpha)\n",
    "    result = pd.concat(result_list)\n",
    "    result = result.reset_index()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(file_location + '/pct1_cal/每日预测/jhjj_data/*.csv')\n",
    "jhjj_data = get_jingjia(filenames)\n",
    "jhjj_data = jhjj_data.sort_values(by=['ticker', 'tradeDate']).reset_index(drop=True)\n",
    "jhjj_data = jhjj_data[jhjj_data.tradeDate >= '2022-07-08']\n",
    "jhjj_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20220805-20220812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_date = '2022-08-04'\n",
    "factors = get_factors(select_date)\n",
    "factors = factors[factors.tradeDate <= '2022-08-10']\n",
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter221_jhjj_data = alter221_jhjj_data[f_index + f_x_221 + extra_factors]\n",
    "old_factors = old_factors.merge(jhjj_data, on=['ticker', 'tradeDate'], how='left')\n",
    "old_factors = old_factors[f_index + f_x_221 + extra_factors]\n",
    "factors = factors[f_index + f_x_221 + _extra_factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([alter221_jhjj_data, old_factors], axis=0)\n",
    "temp.rename({'volume':'jhjj_FACTORS_volume', \n",
    "                                          'cachgPct':'jhjj_FACTORS_cachgPct', \n",
    "                                          'thecommittee':'jhjj_FACTORS_thecommittee', \n",
    "                                          'askVolume1':'jhjj_FACTORS_askVolume1',\n",
    "                                          'bidVolume1':'jhjj_FACTORS_bidVolume1', \n",
    "                                          'caQrr':'jhjj_FACTORS_caQrr', \n",
    "                                          'caTr':'jhjj_FACTORS_caTr', \n",
    "                                          'OCVP1':'jhjj_FACTORS_OCVP1',\n",
    "                                          'Open/vwap-1':'jhjj_FACTORS_Open/vwap-1', \n",
    "                                          'Gap':'jhjj_FACTORS_Gap'}, axis = 1, inplace=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = pd.concat([temp, factors], axis=0)\n",
    "factors = reduce_mem_usage(factors)\n",
    "factors.to_csv(file_location + '/pct1_cal/dailyprediction/gengpanyuce/raw_alter221_jhjj_data_170103_220810.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = factors.tradeDate.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PARALLEL')\n",
    "start = time.time()\n",
    "parallel_obj = Parallel(n_jobs=-1)(delayed(run_df)(f_x, date, factors) for date in dates)\n",
    "print(f'耗时:{time.time() - start}')\n",
    "all_data = pd.concat(parallel_obj)\n",
    "print('sort values')\n",
    "all_data = all_data.sort_values(by=['ticker', 'tradeDate']).reset_index(drop=True)\n",
    "print('reduce memory')\n",
    "all_data = reduce_mem_usage(all_data)\n",
    "all_data.to_csv(file_location + '/pct1_cal/dailyprediction/gengpanyuce/ExcExm_alter221_jhjj_data_170103_220810.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(origin, new):\n",
    "    print('The number of factors of my MODEL: ', len(origin))\n",
    "    count1 = 0\n",
    "    for file in origin:\n",
    "        if file not in new:\n",
    "            print(file)\n",
    "            count1 += 1\n",
    "    print('缺失值', count1)\n",
    "\n",
    "    print('The number of factors in my DATABASE for each day: ', len(new))\n",
    "    count2 = 0\n",
    "    for file in new:\n",
    "        if file not in origin:\n",
    "            print(file)\n",
    "            count2 += 1\n",
    "    print('多余值', count2)\n",
    "\n",
    "    return (count1 == 0 and count2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticekrToStr(job_sum):\n",
    "    job_sum = job_sum[job_sum.ticker<700000]\n",
    "    job_sum.loc[job_sum.ticker<10,'temp']='00000'\n",
    "    job_sum.loc[(job_sum.ticker<100)&(job_sum.ticker>=10),'temp']='0000'\n",
    "    job_sum.loc[(job_sum.ticker<1000)&(job_sum.ticker>=100),'temp']='000'\n",
    "    job_sum.loc[(job_sum.ticker<10000)&(job_sum.ticker>=1000),'temp']='00'\n",
    "    job_sum.loc[job_sum.temp==job_sum.temp,'ticker'] = job_sum[job_sum.temp==job_sum.temp]['temp']+job_sum[job_sum.temp==job_sum.temp]['ticker'].astype(str)\n",
    "    del job_sum['temp']\n",
    "    job_sum['ticker'] = job_sum['ticker'].astype(str)\n",
    "    return job_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcludeExtreme(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.df_median = None\n",
    "        self.df_standard = None\n",
    "\n",
    "    def fit(self, df_columnList):\n",
    "        self.df_median = df_columnList.median()\n",
    "        self.df_standard = df_columnList.apply(lambda x: x - self.df_median[x.name]).abs().median()\n",
    "        return self\n",
    "\n",
    "    def scaller(self, x):\n",
    "        self.di_max = self.df_median[x.name] + 5 * self.df_standard[x.name]\n",
    "        self.di_min = self.df_median[x.name] - 5 * self.df_standard[x.name]\n",
    "        x = x.apply(lambda v: self.di_min if v < self.di_min else v)\n",
    "        x = x.apply(lambda v: self.di_max if v > self.di_max else v)\n",
    "        return x\n",
    "\n",
    "    def transform(self, df_columnList):\n",
    "        df_columnList = df_columnList.apply(self.scaller)\n",
    "        return df_columnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factors(select_date):\n",
    "    \n",
    "    table_name = 'all_Factors'\n",
    "    client = InfluxDBClient(host=\"175.25.50.116\", port=12086, username=\"xtech\", password=\"xtech2022\", database=\"factor\")\n",
    "    result = client.query(\"select * from /\"+ table_name +\"/ where time >= '{}' \".format(select_date))\n",
    "    result = pd.DataFrame(list(result.get_points()))\n",
    "\n",
    "    factors = result.drop(columns= 'time')\n",
    "    factors = factors.rename(columns= {'code': 'ticker'})\n",
    "    factors['ticker'] = factors['ticker'].astype(int)\n",
    "    factors.insert(0, 'tradeDate', factors.pop('tradeDate'))\n",
    "    factors.insert(0, 'ticker', factors.pop('ticker'))\n",
    "    factors['tradeDate'] = factors.tradeDate.apply(lambda x: x[:-9])\n",
    "    factors = factors.dropna(thresh=70)\n",
    "    if 'ga_FACTORS_sfactor175 ' in factors.columns.to_list():\n",
    "        print('Input factor name \\'ga_FACTORS_sfactor175 \\' should be modified.')\n",
    "        factors = factors.rename(columns={'ga_FACTORS_sfactor175 ': 'ga_FACTORS_sfactor175'})\n",
    "    else:\n",
    "        factors['ga_FACTORS_sfactor175'] = np.NAN\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_df(f_x_221, date, df):\n",
    "    print('----- DATE {}------'.format(date))\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    df2 = df[df.tradeDate == date]\n",
    "\n",
    "    mevtransformer = ExcludeExtreme()\n",
    "    mevtransformer.fit(df2[f_x_221])\n",
    "    df2[f_x_221] = mevtransformer.transform(df2[f_x_221])\n",
    "\n",
    "    df2 = df2[f_index + f_x_221]\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = all_data.merge(pct1_label, on=['ticker', 'tradeDate'], how='left')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['pct1_open', 'pct1_open_rank']\n",
    "f_y_list =  label_list # 'pct_dmean'\n",
    "print(f_y_list)\n",
    "#f_x = pickle.load(open(\"pct1_cal/f_x_066\", \"rb\"))\n",
    "\n",
    "#e: extra, m: filted\n",
    "data_source = 'Alter221_extra'\n",
    "this_file_location = file_location + '/pct1_cal/dailyprediction/gengpanyuce'\n",
    "model_list = ['LinearRegression', 'RidgeR', 'DecisionTreeR', 'XGBoostR', 'LGBMRegressor']\n",
    "model_name = model_list[4]\n",
    "target_types = ['r', 'c'] # 分类问题还是回归问题 r 回归问题 c 分类问题\n",
    "target_type = target_types[0]\n",
    "####========需要修改的全局参数========####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df1.tradeDate.sort_values().unique()\n",
    "epoch_ts = list(dates)\n",
    "f_index = ['ticker', 'tradeDate']\n",
    "#result_name = '{}_{}_{}_{}'.format(data_source, model_name, f_y, target_type)\n",
    "#print(result_name)\n",
    "\n",
    "update = 22 # 训练长度：22天\n",
    "train_si = epoch_ts.index('2017-01-03') # included. '2017-01-03'\n",
    "train_ei = epoch_ts.index('2019-01-02') # excluded. '2018-12-28'\n",
    "test_si = epoch_ts.index('2019-01-02') # included. '2019-01-02'\n",
    "test_ei = epoch_ts.index('2019-02-01') # excluded. '2019-01-31' '2019-02-01'\n",
    "test_fi = len(epoch_ts) - 1 # excluded. '2019-01-16'\n",
    "\n",
    "# number of epochs，循环次数\n",
    "num_epoch = round((test_fi - test_ei) / update)\n",
    "epoch_range = range(0, num_epoch + 1)\n",
    "timediff = pd.Timedelta(100,unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lrlist(lr_max,lr_min,num_rounds):\n",
    "    lrlist = [lr_max+(lr_min-lr_max)*(np.log(i)/np.log(num_rounds)) for i in range(1,num_rounds+1)]\n",
    "    return lrlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max = 0.05\n",
    "lr_min = 0.02\n",
    "n_jobs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbm params\n",
    "params =  {\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'num_leaves' : 31,\n",
    "    'feature_fraction' : 0.8,\n",
    "    'bagging_fraction' : 0.8,\n",
    "    'lambda_l1':1,\n",
    "    'lambda_l2':10,\n",
    "    'max_bin' : 64,\n",
    "    'num_boost_round': 200,\n",
    "    'learning_rate' : 0.04,\n",
    "    'min_data_in_leaf':10,\n",
    "    'num_threads': n_jobs\n",
    "}\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = {'l2', 'auc'}\n",
    "lrlist = load_lrlist(lr_max,lr_min,params['num_boost_round'])\n",
    "params['lrlist'] = lrlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f_y in f_y_list:\n",
    "    result_name = '{}_{}_{}_{}'.format(data_source, model_name, f_y, target_type)\n",
    "    print(result_name)\n",
    "\n",
    "    df_result_all = pd.DataFrame()\n",
    "    total_IC = 0\n",
    "    start = time.time()\n",
    "    #epoch_range = range(0, 1)\n",
    "\n",
    "    for epoch in epoch_range:\n",
    "        #epoch = num_epoch + 1\n",
    "        print('----- EPOCH {}------'.format(epoch))\n",
    "        update_n = epoch * update\n",
    "        # get a list of train dates\n",
    "        epoch_t_train = epoch_ts[train_si + update_n : train_ei + update_n]\n",
    "        # get a list of test dates\n",
    "        epoch_t_test = epoch_ts[test_si + update_n : test_ei + update_n]\n",
    "        df_train = df1[df1.tradeDate.apply(lambda x: x in epoch_t_train)].reset_index(drop=True)\n",
    "        df_test = df1[df1.tradeDate.apply(lambda x: x in epoch_t_test)].reset_index(drop=True)\n",
    "        print('预测时间：', epoch_t_test)\n",
    "        print('数据大小：', df_train.shape, df_test.shape)\n",
    "\n",
    "        # 数据筛选 删除上市100天以内的\n",
    "        #a = pd.to_datetime(df_train.tradeDate)\n",
    "        #b = pd.to_datetime(df_train.listData)\n",
    "        #df_train = df_train[a-b > timediff]\n",
    "        #a = pd.to_datetime(df_test.tradeDate)\n",
    "        #b = pd.to_datetime(df_test.listData)\n",
    "        #df_test = df_test[a-b > timediff]\n",
    "\n",
    "        # 获得 x\n",
    "        x_train = df_train[f_x].values\n",
    "        x_test = df_test[f_x].values\n",
    "        print('处理后x:', x_train.shape, x_test.shape)\n",
    "\n",
    "        # 获得y\n",
    "        y_train = df_train[f_y].values\n",
    "        y_test = df_test[f_y].values\n",
    "        #y_train = assign_weight(y_train, weightdic)\n",
    "        #y_test= assign_weight(y_test, weightdic)\n",
    "        #y_train['weight'] = 1\n",
    "        #y_test['weight'] = 1\n",
    "        print('处理后y:', y_train.shape, y_test.shape)\n",
    "        #model = XGBRegressor(n_estimators=863, max_depth=6, learning_rate = 0.0142, subsample = 0.68)\n",
    "        #model = lgb.LGBMRegressor(max_depth=5, num_leaves=10, learning_rate=0.09, n_estimators=100)\n",
    "        #model = lgb.LGBMRegressor(max_depth=6, num_leaves=25, learning_rate=0.1, n_estimators=100)\n",
    "        # {'n_estimators': 100, 'learning_rate': 0.01, 'subsample': 0.7, 'max_depth': 5, 'subsample_freq': 5, 'reg_lambda': 0, 'num_leaves': 10, 'min_child_samples': 80, 'max_bin': 20}\n",
    "        #model = lgb.LGBMRegressor(num_leaves = 15, min_child_samples=200, max_bin = 256, n_estimators=100, \\\n",
    "        #    importance_type='gain', deterministic= True, n_jobs=-1, subsample=0.9, subsample_freq=2)\n",
    "        # 'n_estimators': 100, 'learning_rate': 0.07519120996144206, 'subsample': 0.7489953327400968, 'max_depth': 6, 'subsample_freq': 8, 'reg_lambda': 50, 'num_leaves': 60\n",
    "        #model = lgb.LGBMRegressor(num_leaves= 60, n_estimators=100, learning_rate= 0.07519, reg_lambda = 50,\n",
    "        #subsample= 0.7489953, subsample_freq=8, max_depth=6)\n",
    "        #model = lgb.LGBMRegressor(**params)\n",
    "        #model.fit(x_train, y_train, eval_set=[(x_train,y_train), \n",
    "        #(x_test,y_test)], eval_metric='l2')\n",
    "        lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        model = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=params['num_boost_round'],\n",
    "                        callbacks = [lgb.reset_parameter(learning_rate = params['lrlist'])]\n",
    "                        )\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        # 获得结果\n",
    "        print(f'耗时:{time.time() - start}')\n",
    "        print('get result')\n",
    "        df_result = df_test[f_index].copy()\n",
    "        df_result['y'] = y_test\n",
    "        df_result['y_pred'] = y_pred\n",
    "        IC = df_result[['y', 'y_pred']].corr().iloc[0,1]\n",
    "        print(IC)\n",
    "        df_result_all = df_result_all.append(df_result)\n",
    "\n",
    "    print(f'耗时:{time.time() - start}') \n",
    "    print('sort values')\n",
    "    df_result_all = df_result_all.sort_values(by=['ticker', 'tradeDate']).reset_index(drop=True)\n",
    "    IC = df_result_all[['y', 'y_pred']].corr().iloc[0,1]\n",
    "    today = (datetime.datetime.now()).strftime(\"%Y-%m-%d:%H:%M:%S\")\n",
    "    df_result_all.to_csv('{}/{}{}_{}.csv'.format(this_file_location, result_name, today, round(IC, 4)), index=False)\n",
    "    print('======== COMPLETED {} {} ========'.format(model_name, IC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertFile(filenames, file_location):\n",
    "    for file in filenames:\n",
    "        name = file[:-30] + file[-11:-4]\n",
    "        print(name)\n",
    "        df = pd.read_csv(file_location + file)\n",
    "        df.rename(columns = {'tradeDate': 'date', 'y_pred': 'prediction'}, inplace = True)\n",
    "        df['date'] = df.date.apply(lambda x: str(x).replace('-', ''))\n",
    "        df = df[['ticker', 'date', 'prediction']]\n",
    "\n",
    "        dates = df.date.sort_values().unique()\n",
    "        def run(date):\n",
    "            print(date)\n",
    "            temp = df[df.date == date]\n",
    "            temp = temp.sort_values(by=['prediction'], ascending= False).reset_index(drop=True)\n",
    "            #temp.to_csv('Predictions/zz500_{}/{}.csv'.format(name, date), index=False)\n",
    "            temp.to_csv('{}{}/{}.csv'.format(file_location, name, date), index=False)\n",
    "\n",
    "        print('PARALLEL')\n",
    "        start = time.time()\n",
    "        parallel_obj = Parallel(n_jobs=48)(delayed(run)(date) for date in dates)\n",
    "        print(f'耗时:{time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['Alter221_extra_LGBMRegressor_pct1_open_r2022-08-16:11:15:55_0.1283.csv',\n",
    "'Alter221_extra_LGBMRegressor_pct1_open_rank_r2022-08-16:11:50:58_0.1707.csv']\n",
    "convertFile(filenames, this_file_location + '/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c4e795b307436e90878d87b7b6ea0f9ae1f8d88c9c1ffc8ae37ae0851c943d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
